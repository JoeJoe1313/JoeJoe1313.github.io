---
Title: Metaâ€™s TRImodal Brain Encoder (TRIBE)
Date: 2025-08-21 07:00
Category: Quick Takes
Tags: ai
Slug: 2025-08-21-tribe
Status: published
---

Researchers from Meta have developed **TRIBE** (TRImodal Brain Encoder), the **first** deep encoding pipeline trained across **text** + **audio** + **video**, multiple **cortical areas**, and **individuals**, winning **1st place** in **Algonauts 2025**.

TRIBE fuses features from **Llama-3.2-3B** (text), **Wav2Vec2-BERT** (audio), and **V-JEPA-2** (video) with a **temporal Transformer** and **subject embeddings** to predict **1,000-parcel fMRI** responses to naturalistic videos. The architecture is available in **Figure 1** bloew.

<figure>
  <img src="../images/2025-08-21-tribe/architecture.png" alt="Curation Process" style="display: block; margin: 0 auto; width: 100%">
  <figcaption style="text-align: center">Figure 1. TRIBE architecure</figcaption>
</figure>

### Why itâ€™s cool

&emsp; âœ… Late **fusion of strong unimodal FMs** > unimodal or VL-only baselines

&emsp; âœ… Learns shared dynamics via **subject embeddings**

&emsp; âœ… Robust to missing inputs with **modality dropout**

&emsp; âœ… **Generalizes OOD**: cartoons, documentaries, even silent B&W

### Key results

&emsp; ðŸ¥‡ 1st on **Algonauts 2025** public leaderboard

&emsp; ðŸ“Š Captures **~54% of explainable variance** on average

&emsp; ðŸ”¬ Trained/evaluated on **80+ hours** of fMRI per participant

&emsp; âš¡ **Up to ~30% higher** encoding scores in associative cortices vs the best unimodal model

### Read more

&emsp; ðŸ“„ **Paper:** [https://www.arxiv.org/abs/2507.22229](https://www.arxiv.org/abs/2507.22229)

&emsp; ðŸ’» **Code:** [https://github.com/facebookresearch/algonauts-2025](https://github.com/facebookresearch/algonauts-2025)
